from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import os

# Load environment variables.
load_dotenv()

# Set the model name for our LLMs.
OPENAI_MODEL = "gpt-4o"
# Store the API key in a variable.
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

from langchain.chains import ConversationChain
from langchain.memory import ConversationEntityMemory
from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE

# Initialize the model for entities.
entity_llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,
                        model_name=OPENAI_MODEL,
                        temperature=0.0)

# Initialize the model for output.
chat_llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,
                      model_name=OPENAI_MODEL,
                      temperature=0.7)

# Initialize an object for conversational memory.
buffer = ConversationEntityMemory(llm=entity_llm)

# Create the chain for conversation, using a ConversationBufferMemory object.
conversation = ConversationChain(
    llm=chat_llm, 
    memory=buffer, 
    verbose=False, 
    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE
)

def answer_user_question(question: str) -> str:
    """
    Function to answer user questions by interacting with a conversational chain.
    
    Args:
        question (str): The question or input from the user.
        
    Returns:
        str: The response generated by the AI.
    """
    result = conversation.predict(input=question)
    return result

# Example usage:
if __name__ == "__main__":
    while True:
        user_question = input("You: ")
        if user_question.lower() in ['exit', 'quit', 'stop']:
            print("Conversation ended.")
            break
        
        answer = answer_user_question(user_question)
        print(f"AI: {answer}")
